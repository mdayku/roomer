# Room Detection AI (MVP)

Polygon-aware room detection on blueprints (mock first, model later).

## Overview

This project automates the detection of room boundaries from architectural blueprints. The system accepts blueprint images (PNG/JPG) and returns GeoJSON FeatureCollection with detected room polygons.

## Prerequisites

- Node.js 20+
- pnpm (or npm/yarn)

## Installation

```bash
pnpm install
```

## Development

Start both the backend server and frontend web app:

```bash
pnpm dev
```

This will start:
- **Backend API**: http://localhost:4000
- **Frontend Web**: http://localhost:5173

## Build

```bash
pnpm build
```

## Test

```bash
pnpm test
```

## API Endpoints

### POST `/api/detect`
Upload a blueprint image and receive detected room boundaries.

**Request:**
- Method: `POST`
- Content-Type: `multipart/form-data`
- Body: `image` (file, optional for mock mode)

**Response:**
```json
{
  "type": "FeatureCollection",
  "image": {
    "normalized": true
  },
  "features": [
    {
      "type": "Feature",
      "properties": {
        "id": "room_001",
        "name_hint": "Entry Hall",
        "confidence": 0.95,
        "bbox_norm": [0.05, 0.06, 0.18, 0.24]
      },
      "geometry": {
        "type": "Polygon",
        "coordinates": [[[0.05, 0.06], [0.18, 0.06], [0.18, 0.24], [0.05, 0.24], [0.05, 0.06]]]
      }
    }
  ]
}
```

### GET `/api/health`
Health check endpoint.

**Response:**
```json
{
  "ok": true
}
```

## Frontend

The React frontend provides:
- Image upload interface
- SVG overlay visualization of detected room polygons
- Room labels and hover details

## Project Structure

```
room-detection-ai/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ server/          # Express API backend
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ routes/  # API routes
â”‚   â”‚       â”œâ”€â”€ services/# Detection logic
â”‚   â”‚       â”œâ”€â”€ utils/   # Geometry utilities
â”‚   â”‚       â””â”€â”€ types/    # TypeScript types
â”‚   â””â”€â”€ web/             # React frontend
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ components/
â”‚           â”œâ”€â”€ lib/     # API client
â”‚           â””â”€â”€ types/
â”œâ”€â”€ scripts/             # Utility scripts
â””â”€â”€ package.json         # Root workspace config
```

## Data Format

### Input
- Blueprint images: PNG, JPG
- Coordinates are normalized to [0, 1] range

### Output
- GeoJSON FeatureCollection
- Each feature represents a detected room
- Polygons support arbitrary shapes (not just rectangles)
- Optional bounding box for convenience

## Data Conversion

The project includes data conversion scripts to prepare training data from the CubiCasa5K dataset:

### Convert COCO Annotations (49,522 rooms)
```bash
# Convert COCO format annotations to GeoJSON
pnpm --filter @app/server convert:coco
```

### Convert SVG Vector Data (High-quality polygons)
```bash
# Convert SVG files to GeoJSON
pnpm --filter @app/server convert:svg
```

### Prepare Combined Dataset (Train/Val/Test Split)
```bash
# Combine COCO and SVG data, create train/val/test splits
pnpm --filter @app/server prepare-dataset
```

## Available Scripts

```bash
# Development
pnpm dev                    # Start both backend and frontend
pnpm build                  # Build for production
pnpm test                   # Run tests

# Data pipeline
pnpm --filter @app/server convert:coco        # Convert COCO annotations (49,522 rooms)
pnpm --filter @app/server convert:svg         # Convert SVG files (61,394 rooms)
pnpm --filter @app/server prepare-dataset     # Combine & split into train/val/test (49,536 rooms total)
pnpm --filter @app/server clean-dataset       # Remove any empty collections
pnpm --filter @app/server convert-to-coco     # Convert to ML-friendly COCO format (preserves polygons)

# Training pipeline (Python)
cd room_detection_training
python check_aws.py        # ğŸ” Check AWS credentials first
python sagemaker_train.py  # ğŸš€ YOLO on AWS SageMaker (recommended - 2-4 hours)
python train_yolo.py       # YOLOv8s-seg training locally (2-4 hours)
python train.py            # Mask R-CNN training locally (8-12 hours)
python run_training.py     # Complete local training pipeline
python inference.py        # Test trained model
```

## Project Status

âœ… **Data Pipeline Complete**: 49,536 room polygons in COCO format
âœ… **Training Pipeline Ready**: YOLO11-seg + Mask R-CNN + Cloud SageMaker
âœ… **End-to-End System**: Frontend â†’ API â†’ Mock detection â†’ Visualization
âœ… **Cloud Training Setup**: AWS SageMaker ready for enterprise GPUs

## Next Steps

1. **ğŸš€ ML Training**: Run `cd room_detection_training && python sagemaker_train.py` (2-4 hours)
2. **Performance Optimization**: Ensure <30s inference per blueprint
3. **Model Download**: Get trained model from S3 when complete
4. **Integration Testing**: Connect real ML model to Node.js API
5. **User Acceptance Testing**: Validate against PRD requirements

### Current Architecture

```
Frontend (React) â†’ API (Express) â†’ Mock Detector â†’ Visualizations
                                      â†“
                            Training Pipeline (Python)
                            â†“
                    Mask R-CNN Model (Production)
```

The training pipeline is **production-ready** and designed to meet your PRD requirements for polygon-accurate room detection with sub-30-second inference times.

## License

Private project for Innergy.

